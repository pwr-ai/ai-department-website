@article{radom2021multi,
 abstract = {Multi-task learning (MTL) has been successfully utilized in numerous NLP tasks, including sequence labeling. In this work, we utilize three transformer-based models (XLM-R, HerBERT, mBERT) to improve recognition quality using MTL for selected low-resource language (Polish) and three disjoint sequence labeling tasks with different levels of inter-annotator agreement. Our best MTL model outperforms single-task models both within the tasks domain and overall performance.},
 author = {Radom, Jarema and Koco{\'n}, Jan},
 journal = {Procedia Computer Science},
 pages = {1132--1140},
 pub_year = {2021},
 publisher = {Elsevier},
 title = {Multi-task Sequence Classification for Disjoint Tasks in Low-resource Languages},
 venue = {Procedia Computer Science},
 volume = {192}
}

