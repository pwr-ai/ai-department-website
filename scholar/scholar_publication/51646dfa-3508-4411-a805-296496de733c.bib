@article{kocon2018kgr10,
 abstract = {Description Distributional language model (both textual and binary) for Polish (word embeddings) trained on KGR10 corpus (over 4 billion of words) using Fasttext with the following variants (all possible combinations):-dimension: 100, 300-method: skipgram, cbow-tool: FastText, Magnitude-source text: plain, plain. lower, plain. lemma, plain. lemma. lower The link below leads to the NextCloud directory with all variants of embeddings. If you use it, please cite the following article:@ article {kocon2018embeddings, author={Koco\'{n}, Jan},
 author = {Koco{\'n}, Jan},
 pub_year = {2018},
 publisher = {Wroclaw University of Science and Technology},
 title = {KGR10 FastText polish word embeddings},
 venue = {NA}
}

