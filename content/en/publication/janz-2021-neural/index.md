---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Neural Language Models vs Wordnet-based Semantically Enriched Representation
  in CST Relation Recognition
subtitle: ''
summary: ''
authors:
- Arkadiusz Janz
- Maciej Piasecki
- Piotr WÄ…torski
tags: []
categories: []
date: 2021-01-01
lastmod: 2022-01-12T14:29:02+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-01-12T13:29:02.210128Z'
publication_types:
- '1'
abstract: Neural language models, including transformer-based models, that are pre-trained
  on very large corpora became a common way to represent text in various tasks, including
  recognition of textual semantic relations, eg Cross-document Structure Theory. Pre-trained
  models are usually fine tuned to downstream tasks and the obtained vectors are used
  as an input for deep neural classifiers. No linguistic knowledge obtained from resources
  and tools is utilised. In this paper we compare such universal approaches with a
  combination of rich
publication: '*Proceedings of the 11th Global Wordnet Conference*'
---
